% Encoding: UTF-8

@Software{Trofficus2023,
  author = {Michaelis Trofficus},
  date   = {2023-10-27},
  title  = {gpt4docstrings},
  url    = {https://github.com/MichaelisTrofficus/gpt4docstrings},
}

@Misc{Roziere2024,
  author      = {Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
  date        = {2024},
  title       = {Code Llama: Open Foundation Models for Code},
  eprint      = {2308.12950},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
}

@Online{swimm.io2024,
  date         = {2024},
  editor       = {swimm.io},
  title        = {Code documentation: benefits, challenges, and tips for success},
  url          = {https://swimm.io/learn/code-documentation/code-documentation-benefits-challenges-and-tips-for-success#:~:text=Code documentation is a collection,size of documentation can vary.},
  organization = {Swimm Team},
}

@Online{Stoeffelbauer2023,
  author = {Andreas Stöffelbauer},
  date   = {2023-10-24},
  title  = {How Large Language Models work},
  url    = {https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f},
}

@Online{Beelen2023,
  author = {Jeroen Beelen},
  date   = {2023-06-22},
  title  = {Hoe werken Large Language Models (LLM)?},
  url    = {https://www.linkedin.com/pulse/hoe-werken-large-language-models-llm-jeroen-beelen/?originalSubdomain=nl},
}

@Online{Guinness2024,
  author = {Harry Guinness},
  date   = {2024-01-30},
  title  = {The best large language models (LLMs) in 2024},
  url    = {https://zapier.com/blog/best-llm/},
}

@Online{Google2024,
  author = {Google},
  date   = {2024},
  title  = {Welcome to the Gemini era},
  url    = {https://deepmind.google/technologies/gemini/#introduction},
}

@Online{Meta2024,
  author = {Meta},
  date   = {2024},
  title  = {Llama 2: open source, free for research and commercial use},
  url    = {https://llama.meta.com/},
}

@Online{Das2024,
  author = {Suman Das},
  date   = {2024-01-25},
  title  = {Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA},
  url    = {https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07},
}

@Online{LambertEtAL2022,
  author = {Nathan Lambert and Louis Castricato and Leandro von Werra and Alex Havrilla},
  date   = {2022-12-09},
  title  = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},
  url    = {https://huggingface.co/blog/rlhf},
}

@Software{Doxygen2023,
  author  = {Doxygen},
  date    = {2023-12-25},
  title   = {Code Documentation Automated. Free, open source, cross-platform.},
  url     = {https://www.doxygen.nl/},
  version = {1.10.0},
}

@Online{CodeCat2024,
  author = {CodeCat.AI},
  date   = {2024},
  title  = {AI Docstring Generator},
  url    = {https://www.codecat.ai/ai-docstring-generator},
}

@Online{Corp2023,
  author = {xAI Corp},
  date   = {2023-11-04},
  title  = {Announcing Grok},
  url    = {https://x.ai/},
}

@Online{Anthropic2023,
  author = {Anthropic},
  date   = {2023-05-14},
  title  = {Introducing Claude},
  url    = {https://www.anthropic.com/news/introducing-claude},
}

@Online{aiml2023,
  date  = {2023-11-06},
  title = {What are the primary advantages of transformer models?},
  url   = {https://aiml.com/what-are-the-main-advantages-of-the-transformer-models/},
}

@Online{Hoque2023,
  author = {Minhajul Hoque},
  date   = {2023-04-30},
  title  = {A Comprehensive Overview of Transformer-Based Models: Encoders, Decoders, and More},
  url    = {https://medium.com/@minh.hoque/a-comprehensive-overview-of-transformer-based-models-encoders-decoders-and-more-e9bc0644a4e5},
}

@Online{HashemiPour2024,
  author = {Cameron Hashemi-Pour and Ben Lutkevich},
  date   = {2024-02},
  title  = {BERT language model},
  url    = {https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model},
}

@Online{Cacic2023,
  author = {Miha Cacic},
  date   = {2023-09-06},
  title  = {Pre-training vs Fine-Tuning vs In-Context Learning of Large Language Models},
  url    = {https://www.entrypointai.com/blog/pre-training-vs-fine-tuning-vs-in-context-learning-of-large-language-models/},
}

@Online{ElHousieny2023,
  author = {Rany ElHousieny},
  date   = {2023-11-19},
  title  = {Demystifying Tokenization: Preparing Data for Large Language Models (LLMs)},
  url    = {https://www.linkedin.com/pulse/demystifying-tokenization-preparing-data-large-models-rany-2nebc/},
}

@Online{TeeTracker2023,
  author = {TeeTracker},
  date   = {2023-08-24},
  title  = {LLM fine-tuning step: Tokenizing},
  url    = {https://teetracker.medium.com/llm-fine-tuning-step-tokenizing-caebb280cfc2},
}

@Online{Bailey2024,
  author = {Christopher Bailey},
  date   = {2024},
  title  = {Type Hinting},
  url    = {https://realpython.com/lessons/type-hinting/},
}

@Online{GPT2024,
  author = {{Google Python Team}},
  date   = {2024-02-12},
  title  = {Google Python Style Guide},
  url    = {https://google.github.io/styleguide/pyguide.html},
}

@Comment{jabref-meta: databaseType:biblatex;}
